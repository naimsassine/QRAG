{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber\n",
        "!pip install faiss-cpu\n",
        "!pip install mistralai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rN17zqX4azJf",
        "outputId": "71fcc4af-ee06-407e-8221-a91d2903b5d4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.10/dist-packages (0.11.4)\n",
            "Requirement already satisfied: pdfminer.six==20231228 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (20231228)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (11.0.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (4.30.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.4.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.9.0.post1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: mistralai in /usr/local/lib/python3.10/dist-packages (1.2.3)\n",
            "Requirement already satisfied: eval-type-backport<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from mistralai) (0.2.0)\n",
            "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from mistralai) (0.27.2)\n",
            "Requirement already satisfied: jsonpath-python<2.0.0,>=1.0.6 in /usr/local/lib/python3.10/dist-packages (from mistralai) (1.0.6)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from mistralai) (2.9.2)\n",
            "Requirement already satisfied: python-dateutil==2.8.2 in /usr/local/lib/python3.10/dist-packages (from mistralai) (2.8.2)\n",
            "Requirement already satisfied: typing-inspect<0.10.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from mistralai) (0.9.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil==2.8.2->mistralai) (1.16.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->mistralai) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->mistralai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->mistralai) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->mistralai) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->mistralai) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->mistralai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.9.0->mistralai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.9.0->mistralai) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.9.0->mistralai) (4.12.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<0.10.0,>=0.9.0->mistralai) (1.0.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<0.28.0,>=0.27.0->mistralai) (1.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHBJWr_7Y4Q-",
        "outputId": "119a8132-e296-4583-fd91-c5d9dbc4731e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import io\n",
        "import pdfplumber\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from mistralai import Mistral\n",
        "from google.colab import files\n",
        "\n",
        "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "\n",
        "ATTACHMENT_TYPE_ERR_MSG = (\n",
        "    \"All attachments must be either DctmObjRef or Attachment type, got {}: {}\"\n",
        ")\n",
        "ATTACH_TYPE_EXPECTED = \"Attachment expected to be of type `Attachment`, got {}\"\n",
        "UNEXPECTED_ATTR_TO_PARSE = (\n",
        "    \"Attribute to parse from attachments expected to be in \"\n",
        "    \"['body', 'filename'], got '{}'\"\n",
        ")\n",
        "DCTM_OBJ_REF_EXPECTED = \"Expected DctmObjRef, got {}: {}\"\n",
        "DOXC2TXT_EXCEPTION = \"Cannot process file, raised '{}' error\"\n",
        "LIST_OR_STR_ATTACH_EXPECTED = \"Got type {} for attachment, only list or str accepted\"\n",
        "PAGE_SEP = \"\\n\" + \"=\" * 31 + \" NEW PAGE \" + \"=\" * 31 + \"\\n\"\n",
        "MISSING_SPACES_PATTERNS = [\n",
        "    \"IndicativeTermsheet\\n\",\n",
        "    \"PRIVATEPLACEMENT\\n\",\n",
        "    \"PublicOfferingonlyin:\",\n",
        "]\n",
        "\n",
        "\n",
        "def check_txt_missing_spaces(all_pages_txt: str, threshold: float = 0.06) -> bool:\n",
        "    \"\"\"Check if the parsed PDF has missing spaces (as for all Leonteq termsheets).\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    The alignment used to format the Leonteq termsheets are not properly recognized by\n",
        "    our PDF converter. As an undesirable result, most spaces are being removed during\n",
        "    the conversion step leading to erroneous extractions.\n",
        "    \"\"\"\n",
        "\n",
        "    nb_spaces = all_pages_txt.count(\" \")\n",
        "    nb_chars = len(all_pages_txt)\n",
        "    ratio = nb_spaces / nb_chars\n",
        "\n",
        "    return ratio < threshold and any(\n",
        "        p in all_pages_txt for p in MISSING_SPACES_PATTERNS\n",
        "    )\n",
        "\n",
        "\n",
        "def pdf_text_from_bytes(\n",
        "    pdf_bytes_string: bytes,\n",
        "    max_pages: int = 999,\n",
        "    pages_sep: str = PAGE_SEP,\n",
        ") -> str:\n",
        "    \"\"\"Convert the PDF byte representation to text.\"\"\"\n",
        "    try:\n",
        "        # Pdfplumber returns empty string for UTF-8 encoded strings\n",
        "        # (without any exception raised), only Latin-1 works\n",
        "        # On the other hand, FastAPI requires UTF-8 strings in payloads,\n",
        "        # so we assume UTF-8 string arrives here\n",
        "        pdf_bytes_string = pdf_bytes_string.decode(\"UTF-8\").encode(\"Latin1\")\n",
        "    except UnicodeDecodeError:\n",
        "        # If the above command fails, we will assume the byte string\n",
        "        # is already Latin1 encoded\n",
        "        pass\n",
        "\n",
        "    all_pages_txt = \"\"\n",
        "    all_pages_list = []\n",
        "    with pdfplumber.open(io.BytesIO(pdf_bytes_string)) as pdf:\n",
        "        for page_idx in range(\n",
        "            0, min(len(pdf.pages), max_pages)\n",
        "        ):  # pylint: disable=invalid-name\n",
        "            all_pages_txt += pdf.pages[page_idx].extract_text() + pages_sep\n",
        "            all_pages_list.append(pdf.pages[page_idx].extract_text() + pages_sep)\n",
        "    return all_pages_list\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "\n",
        "    if isinstance(text, list):  # Check if input is a list\n",
        "        text = \" \".join(text)  # Join list elements into a single string\n",
        "\n",
        "    text = text.lower()\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    text = text.replace(\"  \", \" \")\n",
        "    return text\n",
        "\n",
        "\n",
        "# PAGE_SEP constant\n",
        "PAGE_SEP = \"\\n\" + \"=\" * 31 + \" new page \" + \"=\" * 31 + \"\\n\"\n",
        "\n",
        "# Function to extract individual pages from the combined text, ensuring page separators are removed.\n",
        "def extract_pages(all_pages_txt: str) -> list:\n",
        "    \"\"\"Extract individual pages from the combined text.\"\"\"\n",
        "     # Replace page separator with a unique placeholder\n",
        "    placeholder = \"<PAGE_SEPARATOR>\"\n",
        "    clean_text = all_pages_txt.replace(PAGE_SEP.strip(), placeholder)\n",
        "    pages = clean_text.split(placeholder)\n",
        "    pages = [page.strip() for page in pages if page.strip()]\n",
        "    return pages\n",
        "\n",
        "# Function for semantic search\n",
        "def semantic_search(query, model, faiss_index, pages, top_k=5):\n",
        "    \"\"\"Performs semantic search to find relevant pages for a given query.\n",
        "\n",
        "    Args:\n",
        "        query (str): The search query.\n",
        "        model: The sentence transformer model used to encode queries.\n",
        "        faiss_index: The Faiss index to search.\n",
        "        pages: List of extracted pages from the PDF.\n",
        "        top_k (int, optional): The number of top results to return. Defaults to 5.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of indices of the most similar pages in the Faiss index.\n",
        "    \"\"\"\n",
        "    query_embedding = model.encode([query])[0]  # Generate embedding for the query\n",
        "    D, I = faiss_index.search(query_embedding.reshape(1, -1), top_k)  # Search Faiss index\n",
        "\n",
        "    # Filter indices to be within the valid range of 'pages'\n",
        "    relevant_page_indices = [index for index in I[0] if 0 <= index < len(pages)]\n",
        "\n",
        "    return relevant_page_indices\n",
        "\n",
        "\n",
        "# Handle multiple questions and combine relevant pages\n",
        "def combined_semantic_search(questions, pages, model, faiss_index, top_k=3):\n",
        "    \"\"\"Performs semantic search for multiple questions and combines relevant pages.\n",
        "\n",
        "    Args:\n",
        "        questions (list): A list of questions to search for.\n",
        "        pages: List of extracted pages from the PDF.\n",
        "        model: The sentence transformer model used to encode queries.\n",
        "        faiss_index: The Faiss index to search.\n",
        "        top_k (int, optional): The number of top results to return for each question. Defaults to 3.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary where keys are question indices and values are lists of relevant page indices.\n",
        "    \"\"\"\n",
        "    combined_results = {}\n",
        "    for question_index, question in enumerate(questions):\n",
        "        relevant_page_indices = semantic_search(question, model, faiss_index, pages, top_k)\n",
        "        combined_results[question_index] = relevant_page_indices\n",
        "\n",
        "    return combined_results\n",
        "\n",
        "\n",
        "def make_request_mistral(prompt):\n",
        "    api_key = \"nKMmuqxD1WeikspamzeaJRmJOgxBsqsC\"\n",
        "    model = \"mistral-large-latest\"\n",
        "    client = Mistral(api_key=api_key)\n",
        "\n",
        "\n",
        "    chat_response = client.chat.complete(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt,\n",
        "            },\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    return chat_response.choices[0].message.content\n",
        "\n",
        "\n",
        "\n",
        "# Function to extract data from a document\n",
        "def extract_data_from_document(path, user_questions, model, faiss_index, pages):\n",
        "    # Extract individual pages from the cleaned text\n",
        "    #pages = extract_pages(text)\n",
        "\n",
        "    # Perform semantic search to find relevant pages\n",
        "    combined_results = combined_semantic_search(user_questions, pages, model, faiss_index)\n",
        "\n",
        "    # Combine relevant pages' text\n",
        "    relevant_pages_text = \"\"\n",
        "    for question_index, relevant_page_indices in combined_results.items():\n",
        "        for index in relevant_page_indices:\n",
        "            relevant_pages_text += pages[index] + PAGE_SEP\n",
        "\n",
        "\n",
        "    # Build the prompt\n",
        "    pretext = \"\"\"The following is an extract from a document\n",
        "    ---- document beginning ----\n",
        "    \"\"\"\n",
        "    posttext = \"\"\"\n",
        "    ---- document ending ----\n",
        "    Please answer the following question about that document :\n",
        "    \"\"\"\n",
        "    user_questions_text = \"---- question about the document ----\\n\\n\" + \"\\n\\n\".join(\n",
        "        [f\"{i+1}. {q}\" for i, q in enumerate(user_questions)]\n",
        "    )\n",
        "    prompt = pretext + relevant_pages_text + posttext + user_questions_text\n",
        "\n",
        "    # Send prompt to the AI\n",
        "    response = make_request_mistral(prompt)\n",
        "    return response\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def build_faiss_index(pages, model):\n",
        "    \"\"\"\n",
        "    Builds a FAISS index from the given pages and a sentence embedding model.\n",
        "\n",
        "    Args:\n",
        "        pages (list of str): The text of the pages extracted from the document.\n",
        "        model: The sentence transformer model used to embed the pages.\n",
        "\n",
        "    Returns:\n",
        "        faiss_index: A FAISS index containing the embeddings.\n",
        "        page_embeddings: A list of page-level embeddings for future reference.\n",
        "    \"\"\"\n",
        "    # Tokenize and embed each page\n",
        "    tokenized_pages = []\n",
        "    for page in pages:\n",
        "        sentences = page.split(\".\")  # Simple sentence tokenization\n",
        "        embeddings = model.encode(sentences)\n",
        "        tokenized_pages.append(embeddings)\n",
        "\n",
        "    # Generate page-level embeddings (average sentence embeddings)\n",
        "    page_embeddings = []\n",
        "    for page_embedding in tokenized_pages:\n",
        "        page_embedding_avg = np.mean(page_embedding, axis=0)  # Aggregate sentence embeddings\n",
        "        page_embeddings.append(page_embedding_avg)\n",
        "\n",
        "    # Convert to numpy array for FAISS\n",
        "    page_embeddings = np.array(page_embeddings)\n",
        "\n",
        "    # Build the FAISS index with page embeddings\n",
        "    faiss_index = faiss.IndexFlatL2(page_embeddings.shape[1])  # L2 distance index\n",
        "    faiss_index.add(page_embeddings)  # Add page embeddings to FAISS\n",
        "\n",
        "    return faiss_index, page_embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def upload_pdf():\n",
        "  # Step 1: Upload the PDF\n",
        "  print(\"Please upload your PDF file\")\n",
        "  uploaded = files.upload()  # Allows you to upload a file via the Colab interface\n",
        "\n",
        "  # Step 2: Extract the file name\n",
        "  # Get the first uploaded file name\n",
        "  pdf_path = next(iter(uploaded.keys()))\n",
        "  # Open the PDF and extract text\n",
        "  with open(pdf_path, \"rb\") as fobj:\n",
        "      pdf_bytes_utf8 = fobj.read()\n",
        "\n",
        "  # Extract text from the PDF bytes\n",
        "  pdf_text = pdf_text_from_bytes(pdf_bytes_utf8)\n",
        "\n",
        "  # Extract the text and split it into pages\n",
        "  text = clean_text(pdf_text)  # Function to clean the text\n",
        "  pages = extract_pages(text)\n",
        "\n",
        "  # Build FAISS index\n",
        "  faiss_index, page_embeddings = build_faiss_index(pages, model)\n",
        "  return pdf_path, pages, faiss_index\n",
        "\n",
        "def qrag(pdf_path, pages, faiss_index):\n",
        "\n",
        "  # Gather user questions dynamically\n",
        "  while True:\n",
        "    print(\"Enter a question you have about your document : \")\n",
        "    user_questions = []\n",
        "    question = input(\"Question: \")\n",
        "    user_questions.append(question)\n",
        "\n",
        "    if question == \"\":\n",
        "        print(\"No questions provided. Exiting.\")\n",
        "        return None\n",
        "\n",
        "    response_document = extract_data_from_document(pdf_path, user_questions, model, faiss_index, pages)\n",
        "    print(\"Response :\", response_document)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ezLCgjtfcM-h"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path, pages, faiss_index = upload_pdf()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "RFHMdckt8l7M",
        "outputId": "d15d0374-84f8-44e3-f033-f37a63dd0186"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload your PDF file\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3a35f563-3992-4ee0-98c4-40ef1859c497\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3a35f563-3992-4ee0-98c4-40ef1859c497\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving pdfcoffee.com_6-weeks-to-sick-armspdf-pdf-free.pdf to pdfcoffee.com_6-weeks-to-sick-armspdf-pdf-free (7).pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qrag(pdf_path, pages, faiss_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3SZARjj5mxH",
        "outputId": "95f61df3-a4d5-41f6-8d70-80a721188f1a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a question you have about your document : \n",
            "Question: what kind of workout is this?\n",
            "Response : Based on the provided document, this workout is focused on building muscle size and strength, specifically for the arms. The program emphasizes several key techniques and principles:\n",
            "\n",
            "1. **Advanced Training Techniques**: The workout incorporates techniques such as drop sets, forced reps, rest pause, negative reps, and supersets to stimulate muscle growth.\n",
            "\n",
            "2. **Periodization**: The program uses periodization, particularly undulating periodization, which involves constantly switching up weight and rep ranges every workout to keep the muscles growing.\n",
            "\n",
            "3. **Frequent Training**: During weeks three, four, and five, the program involves training arms every other day, which is a form of overreaching designed to stimulate growth without immediately leading to overtraining.\n",
            "\n",
            "4. **Training Splits**: The workout plan alternates training splits each week, involving a 4-day training split with different muscle group pairings each week.\n",
            "\n",
            "Overall, this is a high-intensity, advanced workout program designed to promote significant muscle growth in the arms.\n",
            "Enter a question you have about your document : \n",
            "Question: who is the auther?\n",
            "Response : The document does not explicitly state a single author. It appears to be a collection of comments and discussions from various users on a platform, likely Bodybuilding.com, based on the context and the mention of the website at the end of the document. The users who have posted comments include:\n",
            "\n",
            "- sickendo (Andrei Robert)\n",
            "- eboothe1029 (Evan Boothe)\n",
            "- doubledanger (Danny)\n",
            "- otteruic (Mark)\n",
            "- baigui23\n",
            "- kandmjones (Ken Jones)\n",
            "- sekifegi (Jan Sekne)\n",
            "- biggoals123 (Simon)\n",
            "- scarey2219 (Scott)\n",
            "\n",
            "Additionally, there are references to Dr. Jim Stoppani, who seems to be the creator of a workout program being discussed in the comments. However, Dr. Stoppani is not the author of the document itself.\n",
            "Enter a question you have about your document : \n",
            "Question: \n",
            "No questions provided. Exiting.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zSUKJ7JN6xPS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}